{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Prediction of Cirrhosis Outcomes\n",
    "\n",
    "This notebook demonstrates preprocessing, training, evaluation, and visualization of multiple machine learning models to predict cirrhosis patient outcomes.\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "We firstly need to install the libraries we will use in this project.\n",
    "\n",
    "To do so, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# ML models imports\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_label_encoder.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*ResourceTracker.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94191b71",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "#### Data Loading\n",
    "\n",
    "Loads the dataset from the specified path into a Pandas DataFrame\n",
    "\n",
    "#### ID Column Removal\n",
    "\n",
    "Drops the `id` column as it does not contribute to model training\n",
    "\n",
    "#### Fixing Data Types for Numerical Columns\n",
    "\n",
    "Ensures that specific columns are numeric, converting invalid values to NaN\n",
    "\n",
    "#### Encoding Categorical Variables\n",
    "\n",
    "Encodes categorical variables using `LabelEncoder`\n",
    "Encodes the target column `Status`\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "Imputes missing values in numeric columns using the mean strategy\n",
    "\n",
    "#### Feature Scaling\n",
    "\n",
    "Standardizes numeric columns to have zero mean and unit variance\n",
    "\n",
    "#### Feature-Target Split\n",
    "\n",
    "Separates the dataset into features and target\n",
    "\n",
    "#### Train-Test Split\n",
    "\n",
    "Splits the data into training and testing sets with stratified sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/train.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "numeric_columns_to_fix = ['Cholesterol', 'Copper', 'Tryglicerides', 'Platelets', 'Stage']\n",
    "for col in numeric_columns_to_fix:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "df['Status'] = target_encoder.fit_transform(df['Status'])\n",
    "\n",
    "numeric_cols = ['N_Days', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
    "                'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "X = df.drop('Status', axis=1)\n",
    "y = df['Status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "__all__ = ['X_train', 'X_test', 'y_train', 'y_test', 'target_encoder']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398ab0f",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "\n",
    "#### Function: `evaluate_model`\n",
    "\n",
    "- Evaluates a model with optional cross-validation.\n",
    "- If `cv` is set, performs Stratified K-Fold CV to get mean and std of accuracy, precision, recall, and F1-score.\n",
    "- Trains the model and records training time.\n",
    "- Predicts on test set and records testing time.\n",
    "- Calculates accuracy, precision, recall, F1-score, and confusion matrix on test data.\n",
    "- Returns a dictionary with all these metrics, the trained model, and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, average='macro', cv=None):\n",
    "    results = {}\n",
    "\n",
    "    if cv:\n",
    "        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "        accuracy_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')\n",
    "        precision_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=f'precision_{average}')\n",
    "        recall_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=f'recall_{average}')\n",
    "        f1_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=f'f1_{average}')\n",
    "\n",
    "        results.update({\n",
    "            'cv_accuracy': accuracy_scores.mean(),\n",
    "            'cv_precision': precision_scores.mean(),\n",
    "            'cv_recall': recall_scores.mean(),\n",
    "            'cv_f1_score': f1_scores.mean(),\n",
    "            'cv_std': {\n",
    "                'accuracy': accuracy_scores.std(),\n",
    "                'precision': precision_scores.std(),\n",
    "                'recall': recall_scores.std(),\n",
    "                'f1': f1_scores.std()\n",
    "            },\n",
    "        })\n",
    "\n",
    "    start_train = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_train\n",
    "\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    testing_time = time.time() - start_test\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=average, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    results.update({\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': testing_time,\n",
    "        'confusion_matrix': cm,\n",
    "        'model': model,\n",
    "        'y_pred': y_pred\n",
    "    })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2591b",
   "metadata": {},
   "source": [
    "##  Confusion Matrix Plotting Function\n",
    "\n",
    "####  Function: `plot_confusion_matrix`\n",
    "\n",
    "This function visualizes the confusion matrix of a classification model‚Äôs predictions. It:\n",
    "\n",
    "- Accepts a confusion matrix (`cm`) and an optional model name.\n",
    "- Uses Seaborn‚Äôs heatmap to plot the matrix with annotated counts.\n",
    "- Labels the axes as \"Actual\" (rows) and \"Predicted\" (columns).\n",
    "- Displays class labels: `['C', 'CL', 'D']`.\n",
    "\n",
    "---\n",
    "\n",
    "##  Learning Curve Plotting Function\n",
    "\n",
    "####  Function: `plot_learning_curve`\n",
    "\n",
    "This function plots the learning curve of a model training process. It:\n",
    "\n",
    "- Takes a model, features `X`, labels `y`, and an optional plot title.\n",
    "- Uses 5-fold cross-validation to evaluate training and validation accuracy.\n",
    "- Computes scores for training set sizes from 10% to 100%.\n",
    "- Plots training and validation accuracy vs. training set size.\n",
    "- Helps diagnose underfitting or overfitting by visualizing model performance with varying training data.\n",
    "\n",
    "---\n",
    "\n",
    "##  ROC Curve Plotting Function\n",
    "\n",
    "####  Function: `plot_roc_curve`\n",
    "\n",
    "This function plots the Receiver Operating Characteristic (ROC) curve(s) to evaluate model classification performance. It:\n",
    "\n",
    "- Accepts a trained model, test features `X_test`, true labels `y_test`, and optional model name.\n",
    "- Attempts to get prediction probabilities using `predict_proba`.\n",
    "- If binary classification, plots a single ROC curve with AUC.\n",
    "- If multiclass, plots one ROC curve per class, labeling each with class name and AUC.\n",
    "- Adds a diagonal line for reference (random classifier).\n",
    "- Shows axis labels, legend, and grid for clarity.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, model_name='Model'):\n",
    "    labels = ['C', 'CL', 'D']\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(model, X, y, title='Learning Curve'):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.plot(train_sizes, val_mean, label='Validation score')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(model, X_test, y_test, model_name='Model'):\n",
    "    try:\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        class_labels = ['C', 'CL', 'D']  \n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "\n",
    "        if len(np.unique(y_test)) == 2:\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        else:\n",
    "            for i in range(y_score.shape[1]):\n",
    "                fpr, tpr, _ = roc_curve(y_test == i, y_score[:, i])\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                plt.plot(fpr, tpr, label=f'Status {class_labels[i]} (area = {roc_auc:.2f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except AttributeError:\n",
    "        print(f\"Model {model_name} does not support probability prediction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafb1b3",
   "metadata": {},
   "source": [
    "## Plotting Functions Results\n",
    "\n",
    "#### CatBoost\n",
    "\n",
    "<img src=\"images/Figure_01.png\" width=\"30%\"/>\n",
    "<img src=\"images/Figure_02.png\" width=\"32%\"/>\n",
    "<img src=\"images/Figure_03.png\" width=\"32%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "<img src=\"images/Figure_11.png\" width=\"30%\"/>\n",
    "<img src=\"images/Figure_12.png\" width=\"32%\"/>\n",
    "<img src=\"images/Figure_13.png\" width=\"32%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "<img src=\"images/Figure_21.png\" width=\"30%\"/>\n",
    "<img src=\"images/Figure_22.png\" width=\"32%\"/>\n",
    "<img src=\"images/Figure_23.png\" width=\"32%\"/>\n",
    "\n",
    "---\n",
    "\n",
    "#### StackingClassifier\n",
    "\n",
    "<img src=\"images/Figure_31.png\" width=\"30%\"/>\n",
    "<img src=\"images/Figure_32.png\" width=\"32%\"/>\n",
    "<img src=\"images/Figure_33.png\" width=\"32%\"/>\n",
    "\n",
    "\n",
    "### Analysing the Data\n",
    "\n",
    "#### For Confusion Matrix\n",
    "In general, it is evident that the models are good at identifying Class C (likely the majority class). They struggle significantly with Class CL. Class D performs moderately well but still suffers from some overlap with Class C.\n",
    "\n",
    "#### For Learning Curve\n",
    "\n",
    "The models learn the training data very well (95%+ accuracy), but they perform noticeably worse on unseen data (80%+ accuracy). There's a gap (~15%) between training and validation performance.\n",
    "This suggests that the models likely have enough capacity to fit the data but are slightly overfitting‚Äîmemorizing details from the training set that don‚Äôt generalize well.\n",
    "However, a validation score above 80% on 7,905 samples with 18 features is still quite good.\n",
    "\n",
    "#### For ROC Curve\n",
    "\n",
    "Status C: AUC = 0.9 ‚Üí Excellent discrimination ‚Äî the models can very well distinguish class C from the others.\n",
    "Status CL: AUC = 0.77 ‚Üí Fair discrimination ‚Äî the models are okay but less confident in distinguishing CL from other classes. Could improve.\n",
    "Status D: AUC = 0.9 ‚Üí Excellent discrimination ‚Äî the models are strong at identifying class D.\n",
    "Overall the models perform very well on classes C and D. Class CL is a bit harder to predict confidently because it‚Äôs less represented and overlap with other classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447cea8b",
   "metadata": {},
   "source": [
    "## Main Function: Model Training and Evaluation Pipeline\n",
    "\n",
    "#### Function: `main`\n",
    "\n",
    "- Defines a dictionary of different classifiers with specified parameters.\n",
    "- Iterates over each model, training and evaluating them using the `evaluate_model` function.\n",
    "- Prints accuracy and F1-score for each model.\n",
    "- Visualizes the confusion matrix, ROC curve, and learning curve for each model.\n",
    "- Returns a dictionary containing evaluation results for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    models = {\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            random_state=42, \n",
    "            verbose=0\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            eval_metric='mlogloss',\n",
    "            objective='multi:softmax',\n",
    "            num_class=len(target_encoder.classes_),\n",
    "            random_state=42\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            random_state=42,\n",
    "            max_depth=7,\n",
    "            min_gain_to_split=0,\n",
    "            min_child_samples=20,\n",
    "            verbosity=-1 \n",
    "        )\n",
    "    }\n",
    "\n",
    "    estimators = [\n",
    "        ('svm', SVC(probability=True, random_state=42)),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('mlp', MLPClassifier(random_state=42, max_iter=1000, early_stopping=True)),\n",
    "        ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),\n",
    "        ('dt', DecisionTreeClassifier(max_depth=5, random_state=42))\n",
    "    ]\n",
    "\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(max_iter=200),\n",
    "        passthrough=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    models['StackingClassifier'] = stacking_clf \n",
    "\n",
    "    results_summary = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîç Training and evaluating: {name}\")\n",
    "        results = evaluate_model(\n",
    "            model, \n",
    "            X_train, y_train, \n",
    "            X_test, y_test, \n",
    "            average='macro', \n",
    "            cv=5  \n",
    "        )\n",
    "\n",
    "        print(f\"‚Üí CV Accuracy:  {results['cv_accuracy']:.4f}\")\n",
    "        print(f\"‚Üí CV Precision: {results['cv_precision']:.4f}\")\n",
    "        print(f\"‚Üí CV Recall:    {results['cv_recall']:.4f}\")\n",
    "        print(f\"‚Üí CV F1 Score:  {results['cv_f1_score']:.4f}\")\n",
    "        print(f\"‚Üí Test Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(f\"‚Üí Training Time: {results['training_time']:.2f} s\")\n",
    "        print(f\"‚Üí Testing Time:  {results['testing_time']:.4f} s\")\n",
    "\n",
    "        results_summary[name] = {\n",
    "            'cv_accuracy': results['cv_accuracy'],\n",
    "            'cv_precision': results['cv_precision'],\n",
    "            'cv_recall': results['cv_recall'],\n",
    "            'cv_f1_score': results['cv_f1_score'],\n",
    "            'test_accuracy': results['accuracy'],\n",
    "            'precision': results['precision'],\n",
    "            'recall': results['recall'],\n",
    "            'f1_score': results['f1_score'],\n",
    "            'training_time': results['training_time'],\n",
    "            'testing_time': results['testing_time']\n",
    "        }\n",
    "\n",
    "        plot_confusion_matrix(results['confusion_matrix'], model_name=name)\n",
    "        plot_learning_curve(model, X_train, y_train, title=f'{name} Learning Curve')\n",
    "        plot_roc_curve(model, X_test, y_test, model_name=name)\n",
    "\n",
    "    print(\"\\n=== üßæ Model Performance Summary ===\")\n",
    "    summary_df = pd.DataFrame(results_summary).T.round(4)\n",
    "    print(summary_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
